{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 01 - Data Quality Assessment\n\nComprehensive data quality report for all 9 tables in the Olist e-commerce dataset.\n\n**Tables analyzed:**\n- orders, order_items, order_payments, order_reviews\n- products, customers, sellers, geolocation\n- product_category_translation"
  },
  {
   "cell_type": "markdown",
   "source": "## Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport sqlite3\n\n# Connect to the database\nconn = sqlite3.connect('../data/olist_ecommerce.db')\n\n# List all tables\ntables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\nprint(\"Tables in database:\")\nprint(tables['name'].tolist())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Quality Report Function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def data_quality_report(df, name):\n    print(f\"\\n{'='*60}\")\n    print(f\"DATA QUALITY REPORT: {name}\")\n    print(f\"{'='*60}\")\n    print(f\"Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n    print(f\"\\nNull counts:\")\n    nulls = df.isnull().sum()\n    print(nulls[nulls > 0] if nulls.sum() > 0 else \"No nulls found\")\n    print(f\"\\nNull percentages:\")\n    null_pct = (df.isnull().sum() / len(df) * 100).round(2)\n    print(null_pct[null_pct > 0] if null_pct.sum() > 0 else \"No nulls\")\n    print(f\"\\nDuplicates: {df.duplicated().sum():,}\")\n    print(f\"\\nData types:\\n{df.dtypes}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load All Tables",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load all tables into dataframes\norders = pd.read_sql(\"SELECT * FROM orders\", conn)\norder_items = pd.read_sql(\"SELECT * FROM order_items\", conn)\norder_payments = pd.read_sql(\"SELECT * FROM order_payments\", conn)\norder_reviews = pd.read_sql(\"SELECT * FROM order_reviews\", conn)\nproducts = pd.read_sql(\"SELECT * FROM products\", conn)\ncustomers = pd.read_sql(\"SELECT * FROM customers\", conn)\nsellers = pd.read_sql(\"SELECT * FROM sellers\", conn)\ngeolocation = pd.read_sql(\"SELECT * FROM geolocation\", conn)\nproduct_category_translation = pd.read_sql(\"SELECT * FROM product_category_translation\", conn)\n\nprint(\"All tables loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Quality Reports\n\n### Orders Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(orders, \"orders\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Order Items Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(order_items, \"order_items\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Order Payments Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(order_payments, \"order_payments\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Order Reviews Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(order_reviews, \"order_reviews\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Products Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(products, \"products\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Customers Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(customers, \"customers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Sellers Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(sellers, \"sellers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Geolocation Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(geolocation, \"geolocation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Product Category Translation Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "data_quality_report(product_category_translation, \"product_category_translation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key Findings Summary\n\n### Null Values\n\n| Table | Columns with Nulls | Notes |\n|-------|-------------------|-------|\n| **orders** | `order_approved_at`, `order_delivered_carrier_date`, `order_delivered_customer_date` | Expected for pending/in-transit orders |\n| **order_reviews** | `review_comment_title`, `review_comment_message` | Optional fields - customers can leave rating without comments |\n| **products** | `product_category_name`, `product_name_length`, `product_description_length`, `product_photos_qty`, `product_weight_g`, `product_length_cm`, `product_height_cm`, `product_width_cm` | Some products missing category or dimension data |\n\n### Duplicate Issues\n\n| Table | Duplicate Count | Concern Level |\n|-------|----------------|---------------|\n| **geolocation** | High | Expected - multiple entries per zip code (different lat/lng coordinates within same area) |\n| Others | Low/None | No major concerns |\n\n### Data Type Issues\n\n| Table | Columns | Current Type | Should Be |\n|-------|---------|-------------|-----------|\n| **orders** | All timestamp columns | `object` (string) | `datetime64` |\n| **order_items** | `shipping_limit_date` | `object` (string) | `datetime64` |\n| **order_reviews** | `review_creation_date`, `review_answer_timestamp` | `object` (string) | `datetime64` |\n\n### Important: Customer ID vs Customer Unique ID\n\nThe `customers` table has two ID columns:\n- **`customer_id`**: Unique per order - used to join with orders table\n- **`customer_unique_id`**: Unique per actual customer - use this to count unique customers and track repeat purchases\n\nThis is important for customer analytics - always use `customer_unique_id` when counting unique customers or analyzing customer behavior over time.\n\n### Recommendations for Data Cleaning\n\n1. **Convert timestamp columns** to proper datetime format\n2. **Handle geolocation duplicates** by aggregating (e.g., take mean lat/lng per zip code) when joining\n3. **Handle nulls appropriately**:\n   - Delivery timestamps: Keep as null (represents status)\n   - Review comments: Keep as null (optional fields)\n   - Product dimensions: May need imputation or exclusion for analyses requiring this data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Data Cleaning Implementation\n\nNow that we've assessed data quality, let's implement the cleaning steps.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Step 1: Timestamp Conversion\n\nConvert all timestamp columns in the orders table from strings to datetime objects for time-series analysis.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Convert timestamp columns to datetime\ntimestamp_columns = [\n    'order_purchase_timestamp',\n    'order_approved_at',\n    'order_delivered_carrier_date',\n    'order_delivered_customer_date',\n    'order_estimated_delivery_date'\n]\n\nfor col in timestamp_columns:\n    orders[col] = pd.to_datetime(orders[col])\n\nprint(\"Timestamp columns converted:\")\nprint(orders[timestamp_columns].dtypes)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Step 2: Handle Null Review Comments\n\n**Decision:** Keep null `review_comment_title` and `review_comment_message` as-is.\n\n**Reasoning:** Missing text does not equal a missing review. The `review_score` is the primary metric for sentiment analysis. Many customers leave a star rating without writing a text comment, and this is valid review behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Verify review comments null counts - no action needed, just documenting\nprint(\"Review comments null counts (keeping as-is):\")\nprint(f\"  review_comment_title: {order_reviews['review_comment_title'].isnull().sum():,} nulls\")\nprint(f\"  review_comment_message: {order_reviews['review_comment_message'].isnull().sum():,} nulls\")\nprint(f\"\\nTotal reviews: {len(order_reviews):,}\")\nprint(f\"Reviews with scores (all): {order_reviews['review_score'].notna().sum():,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Step 3: Deduplicate Geolocation\n\nThe geolocation table has many duplicates per zip code (multiple lat/lng readings for the same postal code). For analysis purposes, we keep the first occurrence per zip code.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Deduplicate geolocation - keep first occurrence per zip code\nprint(f\"Original geolocation rows: {len(geolocation):,}\")\nprint(f\"Unique zip codes: {geolocation['geolocation_zip_code_prefix'].nunique():,}\")\n\ngeolocation_clean = geolocation.drop_duplicates(\n    subset=['geolocation_zip_code_prefix'], \n    keep='first'\n)\n\nprint(f\"\\nAfter deduplication: {len(geolocation_clean):,} rows\")\nprint(f\"Rows removed: {len(geolocation) - len(geolocation_clean):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Step 4: Handle Null Product Categories\n\nSome products have null `product_category_name`. We fill these with 'other' to maintain data completeness for category-based analyses.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Fill null product categories with 'other'\nprint(f\"Products with null category: {products['product_category_name'].isnull().sum():,}\")\n\nproducts['product_category_name'] = products['product_category_name'].fillna('other')\n\nprint(f\"\\nAfter filling nulls: {products['product_category_name'].isnull().sum()} nulls\")\nprint(f\"Products now categorized as 'other': {(products['product_category_name'] == 'other').sum():,}\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Step 5: Filter Delivered Orders\n\nFor most analyses, we focus on completed transactions. We create a filtered dataset of delivered orders only.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Filter to delivered orders only\nprint(\"Order status distribution:\")\nprint(orders['order_status'].value_counts())\n\norders_delivered = orders[orders['order_status'] == 'delivered']\n\nprint(f\"\\nTotal orders: {len(orders):,}\")\nprint(f\"Delivered orders: {len(orders_delivered):,}\")\nprint(f\"Percentage delivered: {len(orders_delivered)/len(orders)*100:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Step 6: Understanding customer_id vs customer_unique_id\n\nThe customers table contains two important ID columns that serve different purposes:\n\n| Column | Description | Use Case |\n|--------|-------------|----------|\n| **customer_id** | Unique identifier per order | Use for joining with orders table. Each order has a unique customer_id. |\n| **customer_unique_id** | Unique identifier per person | Use for customer analytics: counting unique customers, repeat purchase analysis, customer lifetime value. |\n\n**Why this matters:** A single person (customer_unique_id) can place multiple orders, each with a different customer_id. If you count customer_id, you count orders. If you count customer_unique_id, you count actual customers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate the difference between customer_id and customer_unique_id\nprint(\"Customer ID analysis:\")\nprint(f\"  Total customer_id entries: {len(customers):,}\")\nprint(f\"  Unique customer_id: {customers['customer_id'].nunique():,}\")\nprint(f\"  Unique customer_unique_id: {customers['customer_unique_id'].nunique():,}\")\nprint(f\"\\nThis means {len(customers) - customers['customer_unique_id'].nunique():,} repeat customers exist in the dataset.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Data Cleaning Summary\n\n| Issue | Table | Resolution | Reasoning |\n|-------|-------|------------|-----------|\n| Timestamps as strings | orders | Convert to datetime | Required for time-series analysis |\n| Null review comments | order_reviews | Keep nulls | Missing text does not equal missing review; review_score is the primary metric |\n| Duplicate geolocations | geolocation | Keep first per zip | Multiple lat/lng readings per zip code |\n| Null category names | products | Fill with 'other' | 610 products missing category |\n| Non-delivered orders | orders | Filter to delivered | Analyzing completed transactions (96,478 of 99,441) |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Summary of cleaned dataframes available for analysis\nprint(\"Cleaned DataFrames Summary:\")\nprint(\"=\"*50)\nprint(f\"orders:           {len(orders):,} rows (timestamps converted)\")\nprint(f\"orders_delivered: {len(orders_delivered):,} rows (delivered only)\")\nprint(f\"order_items:      {len(order_items):,} rows\")\nprint(f\"order_payments:   {len(order_payments):,} rows\")\nprint(f\"order_reviews:    {len(order_reviews):,} rows (nulls preserved)\")\nprint(f\"products:         {len(products):,} rows (nulls filled)\")\nprint(f\"customers:        {len(customers):,} rows\")\nprint(f\"sellers:          {len(sellers):,} rows\")\nprint(f\"geolocation_clean:{len(geolocation_clean):,} rows (deduplicated)\")\nprint(f\"product_category_translation: {len(product_category_translation):,} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Keep connection open for feature engineering section\nprint(\"Data cleaning complete. Proceeding to feature engineering...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Feature Engineering\n\nNow that data is cleaned, we create derived features that add business value for downstream analysis.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Order-Level Features\n\nCreate delivery performance and time-based features for order analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Delivery time features\n# Calculate actual delivery days (from purchase to delivery)\norders['delivery_days'] = (\n    pd.to_datetime(orders['order_delivered_customer_date']) - \n    pd.to_datetime(orders['order_purchase_timestamp'])\n).dt.days\n\n# Calculate estimated delivery days (from purchase to estimated delivery)\norders['estimated_delivery_days'] = (\n    pd.to_datetime(orders['order_estimated_delivery_date']) - \n    pd.to_datetime(orders['order_purchase_timestamp'])\n).dt.days\n\n# Delivery delta: positive = late, negative = early\norders['delivery_delta'] = orders['delivery_days'] - orders['estimated_delivery_days']\n\n# Binary flag for late deliveries (use 'Int64' for nullable integer to handle NaN)\norders['is_late'] = (orders['delivery_delta'] > 0).astype('Int64')\n\nprint(\"Delivery features created:\")\nprint(f\"  delivery_days: {orders['delivery_days'].notna().sum():,} non-null values\")\nprint(f\"  estimated_delivery_days: {orders['estimated_delivery_days'].notna().sum():,} non-null values\")\nprint(f\"  delivery_delta: {orders['delivery_delta'].notna().sum():,} non-null values\")\nprint(f\"  is_late: {orders['is_late'].sum():,} late deliveries ({orders['is_late'].mean()*100:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Time-based features for seasonality and trend analysis\norders['order_month'] = pd.to_datetime(orders['order_purchase_timestamp']).dt.to_period('M')\norders['order_dow'] = pd.to_datetime(orders['order_purchase_timestamp']).dt.day_name()\norders['order_hour'] = pd.to_datetime(orders['order_purchase_timestamp']).dt.hour\n\nprint(\"Time-based features created:\")\nprint(f\"\\nOrders by day of week:\")\nprint(orders['order_dow'].value_counts())\nprint(f\"\\nOrders by month (sample):\")\nprint(orders['order_month'].value_counts().head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Customer-Level Features (RFM Analysis)\n\nRFM (Recency, Frequency, Monetary) analysis is a fundamental customer segmentation technique:\n- **Recency**: Days since last purchase (lower = better)\n- **Frequency**: Number of unique orders (higher = better)\n- **Monetary**: Total spend (higher = better)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Merge orders with customers to get customer_unique_id\norders_customers = orders.merge(customers[['customer_id', 'customer_unique_id']], on='customer_id', how='left')\n\n# Merge with payments to get monetary value\norders_payments = orders_customers.merge(\n    order_payments.groupby('order_id')['payment_value'].sum().reset_index(),\n    on='order_id',\n    how='left'\n)\n\nprint(f\"Merged dataset: {len(orders_payments):,} rows\")\nprint(f\"Unique customers: {orders_payments['customer_unique_id'].nunique():,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate RFM metrics per customer\nsnapshot_date = pd.to_datetime(orders_payments['order_purchase_timestamp']).max()\nprint(f\"Snapshot date (most recent order): {snapshot_date}\")\n\nrfm = orders_payments.groupby('customer_unique_id').agg(\n    recency=('order_purchase_timestamp', lambda x: (snapshot_date - pd.to_datetime(x).max()).days),\n    frequency=('order_id', 'nunique'),\n    monetary=('payment_value', 'sum')\n).reset_index()\n\nprint(f\"\\nRFM DataFrame created: {len(rfm):,} customers\")\nprint(f\"\\nRFM Summary Statistics:\")\nprint(rfm[['recency', 'frequency', 'monetary']].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Merge RFM with customer location data\ncustomers_with_rfm = customers.merge(rfm, on='customer_unique_id', how='left')\n\nprint(f\"Customers with RFM: {len(customers_with_rfm):,} rows\")\nprint(f\"Customers with RFM data: {customers_with_rfm['recency'].notna().sum():,}\")\nprint(f\"\\nSample data:\")\ncustomers_with_rfm[['customer_unique_id', 'customer_city', 'customer_state', 'recency', 'frequency', 'monetary']].head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Product-Level Features\n\nCreate price tiers and freight ratio features for product analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Price tier categorization\norder_items['price_tier'] = pd.cut(\n    order_items['price'],\n    bins=[0, 50, 150, 500, float('inf')],\n    labels=['Budget', 'Mid-Range', 'Premium', 'Luxury']\n)\n\nprint(\"Price tier distribution:\")\nprint(order_items['price_tier'].value_counts())\nprint(f\"\\nPercentage breakdown:\")\nprint((order_items['price_tier'].value_counts(normalize=True) * 100).round(1))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Freight ratio: shipping cost as percentage of product price\n# Handle division by zero for free items\norder_items['freight_ratio'] = order_items['freight_value'] / order_items['price'].replace(0, float('nan'))\n\nprint(\"Freight ratio statistics:\")\nprint(order_items['freight_ratio'].describe())\nprint(f\"\\nItems with freight ratio > 50% (shipping costs more than half the product):\")\nhigh_freight = order_items[order_items['freight_ratio'] > 0.5]\nprint(f\"  Count: {len(high_freight):,} ({len(high_freight)/len(order_items)*100:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Save Processed Data\n\nExport all cleaned and feature-engineered DataFrames to CSV files for downstream analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Define output directory\noutput_dir = '../data/processed/'\nos.makedirs(output_dir, exist_ok=True)\n\n# Save orders with engineered features\n# Convert period to string for CSV compatibility\norders_to_save = orders.copy()\norders_to_save['order_month'] = orders_to_save['order_month'].astype(str)\norders_to_save.to_csv(f'{output_dir}orders_processed.csv', index=False)\nprint(f\"Saved: orders_processed.csv ({len(orders_to_save):,} rows)\")\n\n# Save order items with engineered features\norder_items.to_csv(f'{output_dir}order_items_processed.csv', index=False)\nprint(f\"Saved: order_items_processed.csv ({len(order_items):,} rows)\")\n\n# Save customers with RFM features\ncustomers_with_rfm.to_csv(f'{output_dir}customers_with_rfm.csv', index=False)\nprint(f\"Saved: customers_with_rfm.csv ({len(customers_with_rfm):,} rows)\")\n\n# Save deduplicated geolocation\ngeolocation_clean.to_csv(f'{output_dir}geolocation_clean.csv', index=False)\nprint(f\"Saved: geolocation_clean.csv ({len(geolocation_clean):,} rows)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Engineering Summary\n\n| Feature | DataFrame | Description | Business Value |\n|---------|-----------|-------------|----------------|\n| `delivery_days` | orders | Actual days from purchase to delivery | Measure logistics performance |\n| `estimated_delivery_days` | orders | Promised delivery time | Set customer expectations |\n| `delivery_delta` | orders | Difference between actual and estimated | Identify delivery issues |\n| `is_late` | orders | Binary flag for late deliveries | Track SLA compliance |\n| `order_month` | orders | Month-year of purchase | Seasonality analysis |\n| `order_dow` | orders | Day of week | Weekly pattern analysis |\n| `order_hour` | orders | Hour of purchase | Daily pattern analysis |\n| `recency` | customers_with_rfm | Days since last purchase | Customer engagement |\n| `frequency` | customers_with_rfm | Number of orders | Customer loyalty |\n| `monetary` | customers_with_rfm | Total spend | Customer value |\n| `price_tier` | order_items | Budget/Mid-Range/Premium/Luxury | Product segmentation |\n| `freight_ratio` | order_items | Shipping cost as % of price | Pricing strategy insights |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Final summary of processed files\nprint(\"=\"*60)\nprint(\"DATA CLEANING & FEATURE ENGINEERING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\\nProcessed files saved to: {output_dir}\")\nprint(f\"\\nFiles created:\")\nprint(f\"  1. orders_processed.csv       - Orders with delivery & time features\")\nprint(f\"  2. order_items_processed.csv  - Items with price tier & freight ratio\")\nprint(f\"  3. customers_with_rfm.csv     - Customers with RFM metrics\")\nprint(f\"  4. geolocation_clean.csv      - Deduplicated geolocation data\")\nprint(f\"\\nReady for exploratory data analysis (EDA) in notebook 02.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Close database connection\nconn.close()\nprint(\"Database connection closed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}